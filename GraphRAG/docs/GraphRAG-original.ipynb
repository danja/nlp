{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "105a2123",
   "metadata": {},
   "source": [
    "# Text2Cypher, Graph Retrieval Arguments Generation(RAG), Graph RAG and Graph+Vector RAG\n",
    "\n",
    "To enable Cognitive intelligence App towards private data, RAG + LLM and Knowledge Graph are state-of-the-art approaches.\n",
    "\n",
    "In this demo, we will explain know-how of four types of approaches and compare the trade-off and performance among them.\n",
    "\n",
    "| QueryEngine | Knowledge Graph query engine                                 | Graph RAG query engine                                       | Vector RAG query engine                                      | Graph Vector RAG query engine                                |\n",
    "| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| Mechanism   | 1. **Text-to-GraphQuery** based on KG<br />2. Query KG with the result<br />3. Answer synthesis based on query result | 1. Get related entities of the question<br />2. Get n-depth **SubGraphs** of related entities from KG<br />3. Answer synthesis based on related SubGraphs | 1. Create embedding of question<br />2. Semantic search **top-k related doc chunks**<br />3. Answer synthesis based on related doc chunks | 1. Do retrieval as Vector and Graph RAG <br />2. Answer synthesis based on **both related chunks and SubGraphs** |\n",
    "\n",
    "## Background, RAG\n",
    "\n",
    "Below digrams are showing how RAG works:\n",
    "\n",
    "```\n",
    "                  RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me ....., please   │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ ┌────┐ ┌────┐               │             \n",
    "               │ 3  │ │ 96 │                             \n",
    "             │ └────┘ └────┘               │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```\n",
    "\n",
    "In VectorDB based RAG, we create embeddings of each node(chunk), and find TopK related ones towards a given question during the query. In the above diagram, nodes `3` and `96` were fetched as the TopK related nodes, used to help answer the user query. \n",
    "\n",
    "## Background Graph RAG\n",
    "\n",
    "In Graph RAG, we will extract relationships between entities, representing concise facts from each node. It would look something like this:\n",
    "\n",
    "```\n",
    "Node Split and Embedding\n",
    "\n",
    "┌────┬────┬────┬────┐\n",
    "│ 1  │ 2  │ 3  │ 4  │\n",
    "├────┴────┴────┴────┤\n",
    "│  Docs/Knowledge   │\n",
    "│        ...        │\n",
    "├────┬────┬────┬────┤\n",
    "│ 95 │ 96 │    │    │\n",
    "└────┴────┴────┴────┘\n",
    "```\n",
    "\n",
    "Then, if we zoomed in of it:\n",
    "\n",
    "```\n",
    "       Node Split and Embedding, with Knowledge Graph being extracted\n",
    "\n",
    "┌──────────────────┬──────────────────┬──────────────────┬──────────────────┐\n",
    "│ .─.       .─.    │  .─.       .─.   │            .─.   │  .─.       .─.   │\n",
    "│( x )─────▶ y )   │ ( x )─────▶ a )  │           ( j )  │ ( m )◀────( x )  │\n",
    "│ `▲'       `─'    │  `─'       `─'   │            `─'   │  `─'       `─'   │\n",
    "│  │     1         │        2         │        3    │    │        4         │\n",
    "│ .─.              │                  │            .▼.   │                  │\n",
    "│( z )─────────────┼──────────────────┼──────────▶( i )─┐│                  │\n",
    "│ `◀────┐          │                  │            `─'  ││                  │\n",
    "├───────┼──────────┴──────────────────┴─────────────────┼┴──────────────────┤\n",
    "│       │                      Docs/Knowledge           │                   │\n",
    "│       │                            ...                │                   │\n",
    "│       │                                               │                   │\n",
    "├───────┼──────────┬──────────────────┬─────────────────┼┬──────────────────┤\n",
    "│  .─.  └──────.   │  .─.             │                 ││  .─.             │\n",
    "│ ( x ◀─────( b )  │ ( x )            │                 └┼▶( n )            │\n",
    "│  `─'       `─'   │  `─'             │                  │  `─'             │\n",
    "│        95   │    │   │    96        │                  │   │    98        │\n",
    "│            .▼.   │  .▼.             │                  │   ▼              │\n",
    "│           ( c )  │ ( d )            │                  │  .─.             │\n",
    "│            `─'   │  `─'             │                  │ ( x )            │\n",
    "└──────────────────┴──────────────────┴──────────────────┴──`─'─────────────┘\n",
    "```\n",
    "\n",
    "Where, knowledge, the more granular spliting and information with higher density, optionally multi-hop of `x -> y`, `i -> j -> z -> x` etc... across many more nodes(chunks) than K(in TopK search) could be inlucded in Retrievers. And we believe there are cases that this additional work matters.\n",
    "\n",
    "But how/how well exactly does it work? Let's see in this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75004d1",
   "metadata": {},
   "source": [
    "# 1. Preparation\n",
    "\n",
    "## 1.1 Prepare for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "061a39e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "# For OpenAI\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"INSERT YOUR KEY\"\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "\n",
    "from llama_index import (\n",
    "    KnowledgeGraphIndex,\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import NebulaGraphStore\n",
    "\n",
    "\n",
    "from llama_index.llms import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "# define LLM\n",
    "# NOTE: at the time of demo, text-davinci-002 did not have rate-limit errors\n",
    "llm = OpenAI(temperature=0, model=\"text-davinci-002\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5363496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Azure OpenAI\n",
    "\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index.llms import AzureOpenAI\n",
    "from llama_index import LangchainEmbedding\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    KnowledgeGraphIndex,\n",
    "    ServiceContext,\n",
    ")\n",
    "from llama_index import set_global_service_context\n",
    "\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import NebulaGraphStore\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://<foo-bar>.openai.azure.com\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"youcannottellanyone\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    engine=\"<foo-bar-deployment>\",\n",
    "    temperature=0,\n",
    "    openai_api_version=openai.api_version,\n",
    "    model_kwargs={\n",
    "        \"api_key\": openai.api_key,\n",
    "        \"api_base\": openai.api_base,\n",
    "        \"api_type\": openai.api_type,\n",
    "        \"api_version\": openai.api_version,\n",
    "    },\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embedding_llm = LangchainEmbedding(\n",
    "    OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        deployment=\"<foo-bar-deployment>\",\n",
    "        openai_api_key=openai.api_key,\n",
    "        openai_api_base=openai.api_base,\n",
    "        openai_api_type=openai.api_type,\n",
    "        openai_api_version=openai.api_version,\n",
    "    ),\n",
    "    embed_batch_size=1,\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embedding_llm,\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b17442e",
   "metadata": {},
   "source": [
    "## 1.2. Prepare for NebulaGraph as Graph Store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ca9fa",
   "metadata": {},
   "source": [
    "❗Access NebulaGraph Console to **create space** and **graph schema**\n",
    "\n",
    "```sql\n",
    "CREATE SPACE guardians(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);\n",
    ":sleep 10;\n",
    "USE guardians;\n",
    "CREATE TAG entity(name string);\n",
    "CREATE EDGE relationship(relationship string);\n",
    ":sleep 10;\n",
    "CREATE TAG INDEX entity_index ON entity(name(256));\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6cf0e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nebula3-python in /opt/homebrew/lib/python3.11/site-packages/nebula3_python-3.4.0-py3.11.egg (3.4.0)\n",
      "Requirement already satisfied: ipython-ngql in /opt/homebrew/lib/python3.11/site-packages/ipython_ngql-0.5-py3.11.egg (0.5)\n",
      "Requirement already satisfied: httplib2>=0.20.0 in /opt/homebrew/lib/python3.11/site-packages/httplib2-0.22.0-py3.11.egg (from nebula3-python) (0.22.0)\n",
      "Requirement already satisfied: future>=0.18.0 in /opt/homebrew/lib/python3.11/site-packages/future-0.18.3-py3.11.egg (from nebula3-python) (0.18.3)\n",
      "Requirement already satisfied: six>=1.16.0 in /opt/homebrew/lib/python3.11/site-packages (from nebula3-python) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2021.1 in /opt/homebrew/lib/python3.11/site-packages (from nebula3-python) (2022.7.1)\n",
      "Requirement already satisfied: Jinja2 in /opt/homebrew/lib/python3.11/site-packages/Jinja2-3.1.2-py3.11.egg (from ipython-ngql) (3.1.2)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (from ipython-ngql) (1.5.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/homebrew/lib/python3.11/site-packages/pyparsing-3.1.0b1-py3.11.egg (from httplib2>=0.20.0->nebula3-python) (3.1.0b1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages/MarkupSafe-2.1.2-py3.11-macosx-13-arm64.egg (from Jinja2->ipython-ngql) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->ipython-ngql) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/homebrew/lib/python3.11/site-packages (from pandas->ipython-ngql) (1.24.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nebula3-python ipython-ngql\n",
    "\n",
    "os.environ['NEBULA_USER'] = \"root\"\n",
    "os.environ['NEBULA_PASSWORD'] = \"nebula\" # default password\n",
    "os.environ['NEBULA_ADDRESS'] = \"127.0.0.1:9669\" # assumed we have NebulaGraph installed locally\n",
    "\n",
    "space_name = \"guardians\"\n",
    "edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "tags = [\"entity\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "graph_store = NebulaGraphStore(\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc5613a",
   "metadata": {},
   "source": [
    "## 2. Build the Knowledge Graph\n",
    "\n",
    "In our demo, the Knowledge Graph was created with LLM.\n",
    "\n",
    "We simply do so leveragint the `KnowledgeGraphIndex` from LlamaIndex, when creating it, Triplets will be extracted with LLM and evantually persisted into `NebulaGraphStore`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb7083",
   "metadata": {},
   "source": [
    "### 2.1 Preprocess Data\n",
    "\n",
    "We will download and preprecess data from:\n",
    "    https://en.wikipedia.org/wiki/Guardians_of_the_Galaxy_Vol._3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c376da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "loader = WikipediaReader()\n",
    "\n",
    "documents = loader.load_data(pages=['Guardians of the Galaxy Vol. 3'], auto_suggest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc21c03b",
   "metadata": {},
   "source": [
    "### 2.2 Extract Triplets and Save to NebulaGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f3668",
   "metadata": {},
   "source": [
    "This call will take some time, it'll extract entities and relationships and store them into NebulaGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c05437d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    include_embeddings=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3efdc4",
   "metadata": {},
   "source": [
    "## 3 Create VectorStoreIndex for RAG\n",
    "\n",
    "To compare with/work together with VectorDB based RAG, let's also create a `VectorStoreIndex`.\n",
    "\n",
    "During the creation, same data source will be split into chunks and embedding of them will be created, during the RAG query time, the top-k related embeddings will be vector-searched with the embedding of the question.\n",
    "\n",
    "```\n",
    "                  RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me ....., please   │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ ┌────┐ ┌────┐               │             \n",
    "               │ 3  │ │ 96 │                             \n",
    "             │ └────┘ └────┘               │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```\n",
    "\n",
    "In Llama Index, this could be done with oneline of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "474c2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4c2aee",
   "metadata": {},
   "source": [
    "## 4. Persist and Load from disk Llama Indexes(Optional)\n",
    "\n",
    "Both the `KnowledgeGraphIndex` and `VectorStoreIndex` will be created only once, afterwards, we could persist their in-memory context to enable their reuse from disk anytime.\n",
    "\n",
    "#### Persist\n",
    "\n",
    "```python\n",
    "# persist KG Index(Only MetaData will be persisted, KG is in NebulaGraph)\n",
    "kg_index.storage_context.persist(persist_dir='./storage_graph')\n",
    "\n",
    "# persist Vector Index\n",
    "vector_index.storage_context.persist(persist_dir='./storage_vector')\n",
    "\n",
    "```\n",
    "\n",
    "Then the files are created:\n",
    "\n",
    "```bash\n",
    "$ ls -l ./storage_*\n",
    "\n",
    "storage_graph:\n",
    "total 6384\n",
    "-rw-r--r--@ 1 weyl  staff    44008 Jul 14 11:06 docstore.json\n",
    "-rw-r--r--@ 1 weyl  staff  3219385 Jul 14 11:06 index_store.json\n",
    "-rw-r--r--@ 1 weyl  staff       51 Jul 14 11:06 vector_store.json\n",
    "\n",
    "storage_vector:\n",
    "total 712\n",
    "-rw-r--r--@ 1 weyl  staff   44008 Jul 14 11:06 docstore.json\n",
    "-rw-r--r--@ 1 weyl  staff      18 Jul 14 11:06 graph_store.json\n",
    "-rw-r--r--@ 1 weyl  staff    1003 Jul 14 11:06 index_store.json\n",
    "-rw-r--r--@ 1 weyl  staff  311028 Jul 14 11:06 vector_store.json\n",
    "```\n",
    "\n",
    "#### Restore\n",
    "\n",
    "So we could restore the index from disk like:\n",
    "\n",
    "```python\n",
    "from llama_index import load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir='./storage_graph', graph_store=graph_store)\n",
    "kg_index = load_index_from_storage(\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    include_embeddings=True,\n",
    ")\n",
    "\n",
    "storage_context_vector = StorageContext.from_defaults(persist_dir='./storage_vector')\n",
    "vector_index = load_index_from_storage(\n",
    "    service_context=service_context,\n",
    "    storage_context=storage_context_vector\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd3f4afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "# vector_index.storage_context.persist(persist_dir='./storage_vector')\n",
    "\n",
    "from llama_index import load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir='./storage_graph', graph_store=graph_store)\n",
    "kg_index = load_index_from_storage(\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    include_embeddings=True,\n",
    ")\n",
    "\n",
    "storage_context_vector = StorageContext.from_defaults(persist_dir='./storage_vector')\n",
    "vector_index = load_index_from_storage(\n",
    "    service_context=service_context,\n",
    "    storage_context=storage_context_vector\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2eb936",
   "metadata": {},
   "source": [
    "## 5. Prepare for different query approaches\n",
    "\n",
    "We will do 4 types of query approaches with LLM, KG, VectorDB:\n",
    "\n",
    "| QueryEngine | Knowledge Graph query engine                                 | Graph RAG query engine                                       | Vector RAG query engine                                      | Graph Vector RAG query engine                                |\n",
    "| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| Mechanism   | 1. **Text-to-GraphQuery** based on KG<br />2. Query KG with the result<br />3. Answer synthesis based on query result | 1. Get related entities of the question<br />2. Get n-depth **SubGraphs** of related entities from KG<br />3. Answer synthesis based on related SubGraphs | 1. Create embedding of question<br />2. Semantic search **top-k related doc chunks**<br />3. Answer synthesis based on related doc chunks | 1. Do retrieval as Vector and Graph RAG <br />2. Answer synthesis based on **both related chunks and SubGraphs** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5524b2e",
   "metadata": {},
   "source": [
    "### 5.1 text-to-NebulaGraphCypher\n",
    "\n",
    "Text-to-NebulaGraphCypher approach Translate task/question into a Graph Cypher Query, and answer based on its query result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "097b25c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine import KnowledgeGraphQueryEngine\n",
    "\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import NebulaGraphStore\n",
    "\n",
    "nl2kg_query_engine = KnowledgeGraphQueryEngine(\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c590de2",
   "metadata": {},
   "source": [
    "### 5.2 Graph RAG query engine\n",
    "\n",
    "Graph RAG takes SubGraphs related to entities of the task/question as Context.\n",
    "\n",
    "```\n",
    "           Graph + Vector RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me about x, please │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ Below are knowledge about x │             \n",
    "               x->y<-z,x->h->i, m<-n,...                            \n",
    "             │ Please answer based on them │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b01e372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_rag_query_engine = kg_index.as_query_engine(\n",
    "    include_text=False,\n",
    "    retriever_mode=\"keyword\",\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06682474",
   "metadata": {},
   "source": [
    "### 5.3 Vector RAG query engine\n",
    "\n",
    "Vector RAG is the common approach to find topK semantic related doc chunks as context to synthesize the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37713b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_rag_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93396d7",
   "metadata": {},
   "source": [
    "### 5.4 Graph+Vector RAG query engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0ee74",
   "metadata": {},
   "source": [
    "This is a combined Graph+Vector Based RAG, where we will retrieve both VectorDB and KG SubGraphs as the context, for synthesis of the answer.\n",
    "\n",
    "```\n",
    "           Graph + Vector RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me ....., please   │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ ┌────┐┌────┐               │             \n",
    "               │ 3  ││ 96 │ x->y<-z,x->h...                            \n",
    "             │ └────┘└────┘               │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```\n",
    "\n",
    "To implement that in Llama Index, we create a `CustomRetriever` to comebine the two: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a9516c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QueryBundle\n",
    "from llama_index import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        kg_retriever: KGTableRetriever,\n",
    "        mode: str = \"OR\",\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._kg_retriever = kg_retriever\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        kg_nodes = self._kg_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        kg_ids = {n.node.node_id for n in kg_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in kg_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(kg_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(kg_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe22b2b",
   "metadata": {},
   "source": [
    "Next, we will create instances of the Vector and KG retrievers, which will be used in the instantiation of the Custom Retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecb2d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import get_response_synthesizer\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# create custom retriever\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "kg_retriever = KGTableRetriever(\n",
    "    index=kg_index, retriever_mode=\"keyword\", include_text=False\n",
    ")\n",
    "custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "\n",
    "# create response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd67a63f",
   "metadata": {},
   "source": [
    "And the query engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4976682",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_vector_rag_query_engine = RetrieverQueryEngine(\n",
    "    retriever=custom_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541608be",
   "metadata": {},
   "source": [
    "## 6. Query with all the Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8384be7b",
   "metadata": {},
   "source": [
    "### 6.1 Text-to-GraphQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6acaa1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3mGraph Store Query: MATCH (p:`entity`)-[:relationship]->(e:`entity`) WHERE p.`entity`.`name` == 'Peter Quill' RETURN e.`entity`.`name`;\n",
      "\u001b[0mINFO:llama_index.query_engine.knowledge_graph_query_engine:Graph Store Query: MATCH (p:`entity`)-[:relationship]->(e:`entity`) WHERE p.`entity`.`name` == 'Peter Quill' RETURN e.`entity`.`name`;\n",
      "Graph Store Query: MATCH (p:`entity`)-[:relationship]->(e:`entity`) WHERE p.`entity`.`name` == 'Peter Quill' RETURN e.`entity`.`name`;\n",
      "\u001b[33;1m\u001b[1;3mGraph Store Response: {'e.entity.name': ['Guardians of the Galaxy']}\n",
      "\u001b[0mINFO:llama_index.query_engine.knowledge_graph_query_engine:Graph Store Response: {'e.entity.name': ['Guardians of the Galaxy']}\n",
      "Graph Store Response: {'e.entity.name': ['Guardians of the Galaxy']}\n",
      "\u001b[32;1m\u001b[1;3mFinal Response: \n",
      "Peter Quill is a member of the superhero team known as the Guardians of the Galaxy.\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "Peter Quill is a member of the superhero team known as the Guardians of the Galaxy.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cypher Query:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "```cypher\n",
       "MATCH (p:`entity`)-[:relationship]->(e:`entity`) \n",
       "  WHERE p.`entity`.`name` == 'Peter Quill' \n",
       "RETURN e.`entity`.`name`;\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_nl2kg = nl2kg_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "\n",
    "display(Markdown(f\"<b>{response_nl2kg}</b>\"))\n",
    "\n",
    "# Cypher:\n",
    "\n",
    "print(\"Cypher Query:\")\n",
    "\n",
    "graph_query = nl2kg_query_engine.generate_query(\n",
    "    \"Tell me about Peter Quill?\",\n",
    ")\n",
    "graph_query = graph_query.replace(\"WHERE\", \"\\n  WHERE\").replace(\"RETURN\", \"\\nRETURN\")\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"\n",
    "```cypher\n",
    "{graph_query}\n",
    "```\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bafef8",
   "metadata": {},
   "source": [
    "### 6.2 Graph RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dec5364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.knowledge_graph.retriever:> Starting query: Tell me about Peter Quill.\n",
      "> Starting query: Tell me about Peter Quill.\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Query keywords: ['biography', 'Peter Quill', 'history', 'Peter', 'Quill']\n",
      "> Query keywords: ['biography', 'Peter Quill', 'history', 'Peter', 'Quill']\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'released in', '2014']\n",
      "Peter Quill ['portrays', 'Peter Quill']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'reprised role from', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'directed', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'wrote', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'sequel to', 'Guardians of the Galaxy']\n",
      "Quill ['speaks', ' fuck ']\n",
      "> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'released in', '2014']\n",
      "Peter Quill ['portrays', 'Peter Quill']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'reprised role from', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'directed', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'wrote', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'sequel to', 'Guardians of the Galaxy']\n",
      "Quill ['speaks', ' fuck ']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "Peter Quill is the leader of the Guardians of the Galaxy, a superhero team from Marvel Comics. He was portrayed by Chris Pratt in the 2014 movie of the same name, and reprised his role in the 2017 sequel. He also wrote and directed the first movie. Quill is known for his foul language, often using the word \"fuck\".</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_graph_rag = kg_rag_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b045c800",
   "metadata": {},
   "source": [
    "### 6.3 Vector RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08fa71a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "Peter Quill, also known as Star-Lord, is the half-human, half-Celestial leader of the Guardians of the Galaxy. He was abducted from Earth as a child and raised by a group of alien thieves and smugglers, the Ravagers. In the film, Quill is in a \"state of depression\" following the appearance of a variant of his dead lover Gamora, who does not share the same affection for Quill as her older version had for him, which in turn affects his leadership of the Guardians.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_vector_rag = vector_rag_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f256d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| Result | Graph | Vector |\n",
       "| --- | --- | --- |\n",
       "| Character | Leader of the Guardians of the Galaxy | Half-human, half-Celestial leader of the Guardians of the Galaxy |\n",
       "| Movie | Portrayed by Chris Pratt in the 2014 movie of the same name, and reprised his role in the 2017 sequel. He also wrote and directed the first movie. | In the film, Quill is in a \"state of depression\" following the appearance of a variant of his dead lover Gamora, who does not share the same affection for Quill as her older version had for him, which in turn affects his leadership of the Guardians. |\n",
       "| Personality | Known for his foul language, often using the word \"fuck\". | No mention |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        llm(f\"\"\"\n",
    "Compare the two QA result on \"Tell me about Peter Quill.\", list the differences between them, to help evalute them. Output in markdown table.\n",
    "\n",
    "Result from Graph: {response_graph_rag}\n",
    "---\n",
    "Result from Vector: {response_vector_rag}\n",
    "\n",
    "\"\"\"\n",
    "           )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3e1b07",
   "metadata": {},
   "source": [
    "### 6.4 Graph + Vector RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc59b511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.knowledge_graph.retriever:> Starting query: Tell me about Peter Quill.\n",
      "> Starting query: Tell me about Peter Quill.\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Query keywords: ['biography', 'Peter Quill', 'history', 'Peter', 'Quill']\n",
      "> Query keywords: ['biography', 'Peter Quill', 'history', 'Peter', 'Quill']\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'released in', '2014']\n",
      "Peter Quill ['portrays', 'Peter Quill']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'reprised role from', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'directed', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'wrote', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'sequel to', 'Guardians of the Galaxy']\n",
      "Quill ['speaks', ' fuck ']\n",
      "> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'released in', '2014']\n",
      "Peter Quill ['portrays', 'Peter Quill']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'reprised role from', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'directed', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'wrote', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'sequel to', 'Guardians of the Galaxy']\n",
      "Quill ['speaks', ' fuck ']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "Peter Quill is the half-human, half-Celestial leader of the Guardians of the Galaxy, a group of alien thieves and smugglers. He was abducted from Earth as a child and raised by the Ravagers. Quill reprised his role from the 2014 film Guardians of the Galaxy, which he also directed and wrote. He is the sequel to the original Guardians of the Galaxy and is known to speak with a foul mouth.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_graph_vector_rag = graph_vector_rag_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f82da8",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697e4bc",
   "metadata": {},
   "source": [
    "### 7.1 Overall Comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cbae28",
   "metadata": {},
   "source": [
    "Let's compare the results of them.\n",
    "\n",
    "First check the information that were coverred by different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed1e85e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Markdown' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m display(\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mMarkdown\u001b[49m(\n\u001b[1;32m      3\u001b[0m         llm(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mCompare the QA results on \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me about Peter Quill.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, list the knowledge facts between them, to help evalute them. Output in markdown table.\u001b[39m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mResult text2GraphQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_nl2kg\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m---\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124mResult Graph: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_graph_rag\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m---\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124mResult Vector: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_vector_rag\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m---\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124mResult Graph+Vector: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_graph_vector_rag\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m---\u001b[39m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     16\u001b[0m            )\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     18\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Markdown' is not defined"
     ]
    }
   ],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        llm(f\"\"\"\n",
    "Compare the QA results on \"Tell me about Peter Quill.\", list the knowledge facts between them, to help evalute them. Output in markdown table.\n",
    "\n",
    "Result text2GraphQuery: {response_nl2kg}\n",
    "---\n",
    "Result Graph: {response_graph_rag}\n",
    "---\n",
    "Result Vector: {response_vector_rag}\n",
    "---\n",
    "Result Graph+Vector: {response_graph_vector_rag}\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "           )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b377a880",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "- The pure **KG**(both text2GraphQuery and Graph RAG) comes with **concise** results, and much **lower cost**(for cost comparision see our previous result [here](https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html#comparison-of-results) )\n",
    "- The **Graph+Vector** RAG could be more **comprehensive** in case the question envolves knowledge that's fine-grained **spread** across more chunks than top-K searching.\n",
    "\n",
    "\n",
    "| QueryEngine | Knowledge Graph query engine                                 | Graph RAG query engine                                       | Vector RAG query engine                                      | Graph Vector RAG query engine                                |\n",
    "| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| Mechanism   | 1. **Text-to-GraphQuery** based on KG<br />2. Query KG with the result<br />3. Answer synthesis based on query result | 1. Get related entities of the question<br />2. Get n-depth **SubGraphs** of related entities from KG<br />3. Answer synthesis based on related SubGraphs | 1. Create embedding of question<br />2. Semantic search **top-k related doc chunks**<br />3. Answer synthesis based on related doc chunks | 1. Do retrieval as Vector and Graph RAG <br />2. Answer synthesis based on **both related chunks and SubGraphs** |\n",
    "| Performance | Concise                                                      | Concise                                                      | Fruitful                                                     | Fruitful, could be more comprehensive                        |\n",
    "| Cost        | Low                                                          | Low                                                          | High                                                         | High                                                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ae85ed",
   "metadata": {},
   "source": [
    "### 7.2 Text2GraphQuery vs Graph RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e48a35d",
   "metadata": {},
   "source": [
    "In **Text2GraphQuery**, we leverage the LLM to compose a Graph Query that's trying to provide the answer in the `RETURN` fields. While, on the other hands, the **Graph RAG** find all related knowledges as the context.\n",
    "\n",
    "So when the answer by nature refers to small piece of information as answer, Text2GraphQuery could do the job in the best, and in other cases, the Graph RAG could be better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4510b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| Feature | Text-to-Graph | Graph RAG |\n",
       "| --- | --- | --- |\n",
       "| Superhero Team | Guardians of the Galaxy | Guardians of the Galaxy |\n",
       "| Movie | N/A | 2014 and 2017 |\n",
       "| Role | N/A | Portrayed by Chris Pratt |\n",
       "| Writing/Directing | N/A | Wrote and Directed the first movie |\n",
       "| Character Trait | N/A | Foul language, often using the word \"fuck\" |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        llm(f\"\"\"\n",
    "Compare the two QA result on \"Tell me about Peter Quill.\", list the differences between them, to help evalute them. Output in markdown table.\n",
    "\n",
    "Result from text-to-Graph: {response_nl2kg}\n",
    "---\n",
    "Result from Graph RAG: {response_graph_rag}\n",
    "\n",
    "\"\"\"\n",
    "           )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33632e85",
   "metadata": {},
   "source": [
    "We could visulize the knowledge context that were coverred during the two approach, we could see their difference: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a01d88b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_text2cypher = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "53e78497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n",
      "Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(\"Peter Quill\" :entity{name: \"Peter Quill\"})&lt;-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              path_0\n",
       "0  (\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...\n",
       "1  (\"Peter Quill\" :entity{name: \"Peter Quill\"})<-...\n",
       "2  (\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...\n",
       "3  (\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...\n",
       "4  (\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...\n",
       "5  (\"Peter Quill\" :entity{name: \"Peter Quill\"})-[..."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ngql\n",
    "MATCH path_0=(p:`entity`)-[*1..2]-()\n",
    "  WHERE p.`entity`.`name` == 'Peter Quill' \n",
    "RETURN path_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d05dbbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rag = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "67f8f597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "    <tr>\n",
       "        <th>Text2Cypher Traversed knowledge</th>\n",
       "        <th>Graph Rag Traversed knowledge</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>\n",
       "<iframe\n",
       "    src=\"https://www.siwei.io/demo-dumps/kg-llm/nebulagraph_draw_nl2cypher.html\"\n",
       "    width=450\n",
       "    height=400>\n",
       "</iframe>\n",
       "</td>\n",
       "        <td>\n",
       "<iframe\n",
       "    src=\"https://www.siwei.io/demo-dumps/kg-llm/nebulagraph_draw_rag.html\"\n",
       "    width=450\n",
       "    height=400>\n",
       "</iframe>\n",
       "</td>\n",
       "    </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text2Cypher = \"\"\"\n",
    "<iframe\n",
    "    src=\"https://www.siwei.io/demo-dumps/kg-llm/nebulagraph_draw_nl2cypher.html\"\n",
    "    width=450\n",
    "    height=400>\n",
    "</iframe>\n",
    "\"\"\"\n",
    "graphRAG = \"\"\"\n",
    "<iframe\n",
    "    src=\"https://www.siwei.io/demo-dumps/kg-llm/nebulagraph_draw_rag.html\"\n",
    "    width=450\n",
    "    height=400>\n",
    "</iframe>\n",
    "\"\"\"\n",
    "\n",
    "table = f\"\"\"\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Text2Cypher Traversed knowledge</th>\n",
    "        <th>Graph Rag Traversed knowledge</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>{text2Cypher}</td>\n",
    "        <td>{graphRAG}</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(table))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be0b42",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "For those tasks:\n",
    "\n",
    "- Potentially cares more relationed knowledge\n",
    "- Schema of the KG is sophisticated to be hard for text2cypher to express the task\n",
    "- KG quality isn't good enough\n",
    "- Multiple \"starting entities\" are involved\n",
    "\n",
    "Graph RAG could be a better approach to start with."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
